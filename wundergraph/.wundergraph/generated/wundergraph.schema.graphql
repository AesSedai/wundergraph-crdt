# Code generated by "wunderctl"; DO NOT EDIT.

"""whether this query should be cached (Hasura Cloud only)"""
directive @hasura_cached(
  """refresh the cache entry"""
  refresh: Boolean! = false
  """measured in seconds"""
  ttl: Int! = 60
) on QUERY

directive @fromClaim(name: Claim) on VARIABLE_DEFINITION

"""
The @removeNullVariables directive allows you to remove variables with null value from your GraphQL Query or Mutation Operations.

A potential use-case could be that you have a graphql upstream which is not accepting null values for variables.
By enabling this directive all variables with null values will be removed from upstream query.

query ($say: String, $name: String) @removeNullVariables {
	hello(say: $say, name: $name)
}

Directive will transform variables json and remove top level null values.
{ "say": null, "name": "world" }

So upstream will receive the following variables:

{ "name": "world" }
"""
directive @removeNullVariables on QUERY | MUTATION

directive @hooksVariable on VARIABLE_DEFINITION

directive @jsonSchema(
  """
  The value of both of these keywords MUST be a string.
  
  Both of these keywords can be used to decorate a user interface with
  information about the data produced by this user interface.  A title
  will preferably be short, whereas a description will provide
  explanation about the purpose of the instance described by this
  schema.
  """
  title: String
  """
  The value of both of these keywords MUST be a string.
  
  Both of these keywords can be used to decorate a user interface with
  information about the data produced by this user interface.  A title
  will preferably be short, whereas a description will provide
  explanation about the purpose of the instance described by this
  schema.
  """
  description: String
  """
  The value of "multipleOf" MUST be a number, strictly greater than 0.
  
  A numeric instance is valid only if division by this keyword's value
  results in an integer.
  """
  multipleOf: Int
  """
  The value of "maximum" MUST be a number, representing an inclusive
  upper limit for a numeric instance.
  
  If the instance is a number, then this keyword validates only if the
  instance is less than or exactly equal to "maximum".
  """
  maximum: Int
  """
  The value of "exclusiveMaximum" MUST be number, representing an
  exclusive upper limit for a numeric instance.
  
  If the instance is a number, then the instance is valid only if it
  has a value strictly less than (not equal to) "exclusiveMaximum".
  """
  exclusiveMaximum: Int
  """
  The value of "minimum" MUST be a number, representing an inclusive
  lower limit for a numeric instance.
  
  If the instance is a number, then this keyword validates only if the
  instance is greater than or exactly equal to "minimum".
  """
  minimum: Int
  """
  The value of "exclusiveMinimum" MUST be number, representing an
  exclusive lower limit for a numeric instance.
  
  If the instance is a number, then the instance is valid only if it
  has a value strictly greater than (not equal to) "exclusiveMinimum".
  """
  exclusiveMinimum: Int
  """
  The value of this keyword MUST be a non-negative integer.
  
  A string instance is valid against this keyword if its length is less
  than, or equal to, the value of this keyword.
  
  The length of a string instance is defined as the number of its
  characters as defined by RFC 7159 [RFC7159].
  """
  maxLength: Int
  """
  The value of this keyword MUST be a non-negative integer.
  
  A string instance is valid against this keyword if its length is
  greater than, or equal to, the value of this keyword.
  
  The length of a string instance is defined as the number of its
  characters as defined by RFC 7159 [RFC7159].
  
  Omitting this keyword has the same behavior as a value of 0.
  """
  minLength: Int
  """
  The value of this keyword MUST be a string.  This string SHOULD be a
  valid regular expression, according to the ECMA 262 regular
  expression dialect.
  
  A string instance is considered valid if the regular expression
  matches the instance successfully.  Recall: regular expressions are
  not implicitly anchored.
  """
  pattern: String
  """
  The value of this keyword MUST be a non-negative integer.
  
  An array instance is valid against "maxItems" if its size is less
  than, or equal to, the value of this keyword.
  """
  maxItems: Int
  """
  The value of this keyword MUST be a non-negative integer.
  
  An array instance is valid against "minItems" if its size is greater
  than, or equal to, the value of this keyword.
  
  Omitting this keyword has the same behavior as a value of 0.
  """
  minItems: Int
  """
  The value of this keyword MUST be a boolean.
  
  If this keyword has boolean value false, the instance validates
  successfully.  If it has boolean value true, the instance validates
  successfully if all of its elements are unique.
  
  Omitting this keyword has the same behavior as a value of false.
  """
  uniqueItems: Boolean
  commonPattern: COMMON_REGEX_PATTERN
) on VARIABLE_DEFINITION

directive @rbac(
  """the user must match all roles"""
  requireMatchAll: [WG_ROLE]
  """the user must match at least one of the roles"""
  requireMatchAny: [WG_ROLE]
  """the user must not match all of the roles"""
  denyMatchAll: [WG_ROLE]
  """the user must not match any of the roles"""
  denyMatchAny: [WG_ROLE]
) on QUERY | MUTATION | SUBSCRIPTION

"""
The directive @injectCurrentDateTime injects a DateTime string of the current date and time into the variable.
This variable MUST be a string compatible scalar. 

The default format, is: ISO 8601
If no format is chosen, the default format is used.
Custom formats are allowed by specifying a format conforming to the Golang specification for specifying a date time format.
"""
directive @injectCurrentDateTime(
  format: WunderGraphDateTimeFormat = ISO8601
  """
  customFormat must conform to the Golang specification for specifying a date time format
  """
  customFormat: String
) on VARIABLE_DEFINITION

"""
The directive @injectGeneratedUUID injects a generated UUID into the variable.
This variable MUST be a string.
At the same time, it removes the variable from the input definition,
disallowing the user to supply it.

This means, the UUID is 100% generated server-side and can be considered untempered.
"""
directive @injectGeneratedUUID on VARIABLE_DEFINITION

"""
The @internalOperation Directive marks an Operation as internal.
By doing so, the Operation is no longer accessible from the public API.
It can only be accessed by internal services, like hooks.
"""
directive @internalOperation on QUERY | MUTATION | SUBSCRIPTION

"""
The directive @injectEnvironmentVariable allows you to inject an environment variable into the variable definition.
"""
directive @injectEnvironmentVariable(name: String!) on VARIABLE_DEFINITION

"""
The @export directive instructs the Execution Planner to export the field during the execution into the variable of the 'as' argument.
As the execution is depth first, a field can only be used after it has been exported.
Additionally, a field can only be used after using the '_join' field or on a different data source.
It's not possible to export a field and use it in for the same data source.

Note that the @export directive only works on fields that return a single value.
It's not possible to export a list or object field.
"""
directive @export(
  """The argument 'as' is the name of the variable to export the field to."""
  as: String!
) on FIELD

"""
The directive @internal marks a variable definition as internal so that clients can't access it.
The field is also not visible in the public API.
It's only being used as an internal variable to export fields into.
"""
directive @internal on VARIABLE_DEFINITION

"""
The @transform directive allows to apply transformations to the response.
By applying the directive, the shape of the response can be altered,
which will also modify the JSON-Schema of the response.
That is, you will keep full type safety and code-generation for transformed fields.
"""
directive @transform(
  """
  Using the 'get' transformation allows you to extract a nested field using a JSON path.
  This is useful to unnest data, e.g. when using the '_join' field, which adds an extra layer of nesting.
  
  Example:
  
  query GetName {
      name: me @transform(get: "info.name") {
          info {
              name
          }
      }
  }
  
  Before the transformation, the resolve looks like this:
  
  {
      "name": {
          "info": {
              "name": "John Doe"
          }
      }
  }
  
  With the transformation applied, the response will be reshaped like this:
  
  {
      "name": "John Doe"
  }
  """
  get: String
) on FIELD

"""
Boolean expression to compare columns of type "Int". All fields are combined with logical 'AND'.
"""
input hasura_Int_comparison_exp {
  _eq: Int
  _gt: Int
  _gte: Int
  _in: [Int!]
  _is_null: Boolean
  _lt: Int
  _lte: Int
  _neq: Int
  _nin: [Int!]
}

"""
Boolean expression to compare columns of type "String". All fields are combined with logical 'AND'.
"""
input hasura_String_comparison_exp {
  _eq: String
  _gt: String
  _gte: String
  """does the column match the given case-insensitive pattern"""
  _ilike: String
  _in: [String!]
  """
  does the column match the given POSIX regular expression, case insensitive
  """
  _iregex: String
  _is_null: Boolean
  """does the column match the given pattern"""
  _like: String
  _lt: String
  _lte: String
  _neq: String
  """does the column NOT match the given case-insensitive pattern"""
  _nilike: String
  _nin: [String!]
  """
  does the column NOT match the given POSIX regular expression, case insensitive
  """
  _niregex: String
  """does the column NOT match the given pattern"""
  _nlike: String
  """
  does the column NOT match the given POSIX regular expression, case sensitive
  """
  _nregex: String
  """does the column NOT match the given SQL regular expression"""
  _nsimilar: String
  """
  does the column match the given POSIX regular expression, case sensitive
  """
  _regex: String
  """does the column match the given SQL regular expression"""
  _similar: String
}

"""
columns and relationships of "authors"
"""
type hasura_authors {
  """An array relationship"""
  books(
    """distinct select on columns"""
    distinct_on: [hasura_books_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_books_order_by!]
    """filter the rows returned"""
    where: hasura_books_bool_exp
  ): [hasura_books!]!
  """An aggregate relationship"""
  books_aggregate(
    """distinct select on columns"""
    distinct_on: [hasura_books_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_books_order_by!]
    """filter the rows returned"""
    where: hasura_books_bool_exp
  ): hasura_books_aggregate!
  created_at: hasura_timestamptz!
  id: String!
  name: String
  updated_at: hasura_timestamptz!
  _join: Query!
}

"""
aggregated selection of "authors"
"""
type hasura_authors_aggregate {
  aggregate: hasura_authors_aggregate_fields
  nodes: [hasura_authors!]!
  _join: Query!
}

"""
aggregate fields of "authors"
"""
type hasura_authors_aggregate_fields {
  count(columns: [hasura_authors_select_column!], distinct: Boolean): Int!
  max: hasura_authors_max_fields
  min: hasura_authors_min_fields
  _join: Query!
}

"""
Boolean expression to filter rows from the table "authors". All fields are combined with a logical 'AND'.
"""
input hasura_authors_bool_exp {
  _and: [hasura_authors_bool_exp!]
  _not: hasura_authors_bool_exp
  _or: [hasura_authors_bool_exp!]
  books: hasura_books_bool_exp
  books_aggregate: hasura_books_aggregate_bool_exp
  created_at: hasura_timestamptz_comparison_exp
  id: hasura_String_comparison_exp
  name: hasura_String_comparison_exp
  updated_at: hasura_timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "authors"
"""
enum hasura_authors_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  authors_pkey
}

"""
input type for inserting data into table "authors"
"""
input hasura_authors_insert_input {
  books: hasura_books_arr_rel_insert_input
  created_at: hasura_timestamptz
  id: String
  name: String
  updated_at: hasura_timestamptz
}

"""aggregate max on columns"""
type hasura_authors_max_fields {
  created_at: hasura_timestamptz
  id: String
  name: String
  updated_at: hasura_timestamptz
  _join: Query!
}

"""aggregate min on columns"""
type hasura_authors_min_fields {
  created_at: hasura_timestamptz
  id: String
  name: String
  updated_at: hasura_timestamptz
  _join: Query!
}

"""
response of any mutation on the table "authors"
"""
type hasura_authors_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!
  """data from the rows affected by the mutation"""
  returning: [hasura_authors!]!
  _join: Query!
}

"""
input type for inserting object relation for remote table "authors"
"""
input hasura_authors_obj_rel_insert_input {
  data: hasura_authors_insert_input!
  """upsert condition"""
  on_conflict: hasura_authors_on_conflict
}

"""
on_conflict condition type for table "authors"
"""
input hasura_authors_on_conflict {
  constraint: hasura_authors_constraint!
  update_columns: [hasura_authors_update_column!]! = []
  where: hasura_authors_bool_exp
}

"""Ordering options when selecting data from "authors"."""
input hasura_authors_order_by {
  books_aggregate: hasura_books_aggregate_order_by
  created_at: hasura_order_by
  id: hasura_order_by
  name: hasura_order_by
  updated_at: hasura_order_by
}

"""primary key columns input for table: authors"""
input hasura_authors_pk_columns_input {
  id: String!
}

"""
select columns of table "authors"
"""
enum hasura_authors_select_column {
  """column name"""
  created_at
  """column name"""
  id
  """column name"""
  name
  """column name"""
  updated_at
}

"""
input type for updating data in table "authors"
"""
input hasura_authors_set_input {
  created_at: hasura_timestamptz
  id: String
  name: String
  updated_at: hasura_timestamptz
}

"""
Streaming cursor of the table "authors"
"""
input hasura_authors_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: hasura_authors_stream_cursor_value_input!
  """cursor ordering"""
  ordering: hasura_cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input hasura_authors_stream_cursor_value_input {
  created_at: hasura_timestamptz
  id: String
  name: String
  updated_at: hasura_timestamptz
}

"""
update columns of table "authors"
"""
enum hasura_authors_update_column {
  """column name"""
  created_at
  """column name"""
  id
  """column name"""
  name
  """column name"""
  updated_at
}

input hasura_authors_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: hasura_authors_set_input
  where: hasura_authors_bool_exp!
}

"""
columns and relationships of "books"
"""
type hasura_books {
  """An object relationship"""
  author: hasura_authors
  author_id: String
  created_at: hasura_timestamptz!
  id: String!
  isbn: String
  published_at: hasura_timestamptz
  title: String
  updated_at: hasura_timestamptz!
  _join: Query!
}

"""
aggregated selection of "books"
"""
type hasura_books_aggregate {
  aggregate: hasura_books_aggregate_fields
  nodes: [hasura_books!]!
  _join: Query!
}

input hasura_books_aggregate_bool_exp {
  count: hasura_books_aggregate_bool_exp_count
}

input hasura_books_aggregate_bool_exp_count {
  arguments: [hasura_books_select_column!]
  distinct: Boolean
  filter: hasura_books_bool_exp
  predicate: hasura_Int_comparison_exp!
}

"""
aggregate fields of "books"
"""
type hasura_books_aggregate_fields {
  count(columns: [hasura_books_select_column!], distinct: Boolean): Int!
  max: hasura_books_max_fields
  min: hasura_books_min_fields
  _join: Query!
}

"""
order by aggregate values of table "books"
"""
input hasura_books_aggregate_order_by {
  count: hasura_order_by
  max: hasura_books_max_order_by
  min: hasura_books_min_order_by
}

"""
input type for inserting array relation for remote table "books"
"""
input hasura_books_arr_rel_insert_input {
  data: [hasura_books_insert_input!]!
  """upsert condition"""
  on_conflict: hasura_books_on_conflict
}

"""
Boolean expression to filter rows from the table "books". All fields are combined with a logical 'AND'.
"""
input hasura_books_bool_exp {
  _and: [hasura_books_bool_exp!]
  _not: hasura_books_bool_exp
  _or: [hasura_books_bool_exp!]
  author: hasura_authors_bool_exp
  author_id: hasura_String_comparison_exp
  created_at: hasura_timestamptz_comparison_exp
  id: hasura_String_comparison_exp
  isbn: hasura_String_comparison_exp
  published_at: hasura_timestamptz_comparison_exp
  title: hasura_String_comparison_exp
  updated_at: hasura_timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "books"
"""
enum hasura_books_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  books_pkey
}

"""
input type for inserting data into table "books"
"""
input hasura_books_insert_input {
  author: hasura_authors_obj_rel_insert_input
  author_id: String
  created_at: hasura_timestamptz
  id: String
  isbn: String
  published_at: hasura_timestamptz
  title: String
  updated_at: hasura_timestamptz
}

"""aggregate max on columns"""
type hasura_books_max_fields {
  author_id: String
  created_at: hasura_timestamptz
  id: String
  isbn: String
  published_at: hasura_timestamptz
  title: String
  updated_at: hasura_timestamptz
  _join: Query!
}

"""
order by max() on columns of table "books"
"""
input hasura_books_max_order_by {
  author_id: hasura_order_by
  created_at: hasura_order_by
  id: hasura_order_by
  isbn: hasura_order_by
  published_at: hasura_order_by
  title: hasura_order_by
  updated_at: hasura_order_by
}

"""aggregate min on columns"""
type hasura_books_min_fields {
  author_id: String
  created_at: hasura_timestamptz
  id: String
  isbn: String
  published_at: hasura_timestamptz
  title: String
  updated_at: hasura_timestamptz
  _join: Query!
}

"""
order by min() on columns of table "books"
"""
input hasura_books_min_order_by {
  author_id: hasura_order_by
  created_at: hasura_order_by
  id: hasura_order_by
  isbn: hasura_order_by
  published_at: hasura_order_by
  title: hasura_order_by
  updated_at: hasura_order_by
}

"""
response of any mutation on the table "books"
"""
type hasura_books_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!
  """data from the rows affected by the mutation"""
  returning: [hasura_books!]!
  _join: Query!
}

"""
on_conflict condition type for table "books"
"""
input hasura_books_on_conflict {
  constraint: hasura_books_constraint!
  update_columns: [hasura_books_update_column!]! = []
  where: hasura_books_bool_exp
}

"""Ordering options when selecting data from "books"."""
input hasura_books_order_by {
  author: hasura_authors_order_by
  author_id: hasura_order_by
  created_at: hasura_order_by
  id: hasura_order_by
  isbn: hasura_order_by
  published_at: hasura_order_by
  title: hasura_order_by
  updated_at: hasura_order_by
}

"""primary key columns input for table: books"""
input hasura_books_pk_columns_input {
  id: String!
}

"""
select columns of table "books"
"""
enum hasura_books_select_column {
  """column name"""
  author_id
  """column name"""
  created_at
  """column name"""
  id
  """column name"""
  isbn
  """column name"""
  published_at
  """column name"""
  title
  """column name"""
  updated_at
}

"""
input type for updating data in table "books"
"""
input hasura_books_set_input {
  author_id: String
  created_at: hasura_timestamptz
  id: String
  isbn: String
  published_at: hasura_timestamptz
  title: String
  updated_at: hasura_timestamptz
}

"""
Streaming cursor of the table "books"
"""
input hasura_books_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: hasura_books_stream_cursor_value_input!
  """cursor ordering"""
  ordering: hasura_cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input hasura_books_stream_cursor_value_input {
  author_id: String
  created_at: hasura_timestamptz
  id: String
  isbn: String
  published_at: hasura_timestamptz
  title: String
  updated_at: hasura_timestamptz
}

"""
update columns of table "books"
"""
enum hasura_books_update_column {
  """column name"""
  author_id
  """column name"""
  created_at
  """column name"""
  id
  """column name"""
  isbn
  """column name"""
  published_at
  """column name"""
  title
  """column name"""
  updated_at
}

input hasura_books_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: hasura_books_set_input
  where: hasura_books_bool_exp!
}

"""
columns and relationships of "crdt"
"""
type hasura_crdt {
  client: String
  created_at: hasura_timestamptz!
  id: String!
  state: String
  updated_at: hasura_timestamptz!
  vector: String
  _join: Query!
}

"""
aggregated selection of "crdt"
"""
type hasura_crdt_aggregate {
  aggregate: hasura_crdt_aggregate_fields
  nodes: [hasura_crdt!]!
  _join: Query!
}

"""
aggregate fields of "crdt"
"""
type hasura_crdt_aggregate_fields {
  count(columns: [hasura_crdt_select_column!], distinct: Boolean): Int!
  max: hasura_crdt_max_fields
  min: hasura_crdt_min_fields
  _join: Query!
}

"""
Boolean expression to filter rows from the table "crdt". All fields are combined with a logical 'AND'.
"""
input hasura_crdt_bool_exp {
  _and: [hasura_crdt_bool_exp!]
  _not: hasura_crdt_bool_exp
  _or: [hasura_crdt_bool_exp!]
  client: hasura_String_comparison_exp
  created_at: hasura_timestamptz_comparison_exp
  id: hasura_String_comparison_exp
  state: hasura_String_comparison_exp
  updated_at: hasura_timestamptz_comparison_exp
  vector: hasura_String_comparison_exp
}

"""
unique or primary key constraints on table "crdt"
"""
enum hasura_crdt_constraint {
  """
  unique or primary key constraint on columns "client"
  """
  crdt_client_unique
  """
  unique or primary key constraint on columns "id"
  """
  crdt_pkey
}

"""
input type for inserting data into table "crdt"
"""
input hasura_crdt_insert_input {
  client: String
  created_at: hasura_timestamptz
  id: String
  state: String
  updated_at: hasura_timestamptz
  vector: String
}

"""aggregate max on columns"""
type hasura_crdt_max_fields {
  client: String
  created_at: hasura_timestamptz
  id: String
  state: String
  updated_at: hasura_timestamptz
  vector: String
  _join: Query!
}

"""aggregate min on columns"""
type hasura_crdt_min_fields {
  client: String
  created_at: hasura_timestamptz
  id: String
  state: String
  updated_at: hasura_timestamptz
  vector: String
  _join: Query!
}

"""
response of any mutation on the table "crdt"
"""
type hasura_crdt_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!
  """data from the rows affected by the mutation"""
  returning: [hasura_crdt!]!
  _join: Query!
}

"""
on_conflict condition type for table "crdt"
"""
input hasura_crdt_on_conflict {
  constraint: hasura_crdt_constraint!
  update_columns: [hasura_crdt_update_column!]! = []
  where: hasura_crdt_bool_exp
}

"""Ordering options when selecting data from "crdt"."""
input hasura_crdt_order_by {
  client: hasura_order_by
  created_at: hasura_order_by
  id: hasura_order_by
  state: hasura_order_by
  updated_at: hasura_order_by
  vector: hasura_order_by
}

"""primary key columns input for table: crdt"""
input hasura_crdt_pk_columns_input {
  id: String!
}

"""
select columns of table "crdt"
"""
enum hasura_crdt_select_column {
  """column name"""
  client
  """column name"""
  created_at
  """column name"""
  id
  """column name"""
  state
  """column name"""
  updated_at
  """column name"""
  vector
}

"""
input type for updating data in table "crdt"
"""
input hasura_crdt_set_input {
  client: String
  created_at: hasura_timestamptz
  id: String
  state: String
  updated_at: hasura_timestamptz
  vector: String
}

"""
Streaming cursor of the table "crdt"
"""
input hasura_crdt_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: hasura_crdt_stream_cursor_value_input!
  """cursor ordering"""
  ordering: hasura_cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input hasura_crdt_stream_cursor_value_input {
  client: String
  created_at: hasura_timestamptz
  id: String
  state: String
  updated_at: hasura_timestamptz
  vector: String
}

"""
update columns of table "crdt"
"""
enum hasura_crdt_update_column {
  """column name"""
  client
  """column name"""
  created_at
  """column name"""
  id
  """column name"""
  state
  """column name"""
  updated_at
  """column name"""
  vector
}

input hasura_crdt_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: hasura_crdt_set_input
  where: hasura_crdt_bool_exp!
}

"""ordering argument of a cursor"""
enum hasura_cursor_ordering {
  """ascending ordering of the cursor"""
  ASC
  """descending ordering of the cursor"""
  DESC
}

"""mutation root"""
type Mutation {
  """
  delete data from the table: "authors"
  """
  hasura_delete_authors(
    """filter the rows which have to be deleted"""
    where: hasura_authors_bool_exp!
  ): hasura_authors_mutation_response
  """
  delete single row from the table: "authors"
  """
  hasura_delete_authors_by_pk(id: String!): hasura_authors
  """
  delete data from the table: "books"
  """
  hasura_delete_books(
    """filter the rows which have to be deleted"""
    where: hasura_books_bool_exp!
  ): hasura_books_mutation_response
  """
  delete single row from the table: "books"
  """
  hasura_delete_books_by_pk(id: String!): hasura_books
  """
  delete data from the table: "crdt"
  """
  hasura_delete_crdt(
    """filter the rows which have to be deleted"""
    where: hasura_crdt_bool_exp!
  ): hasura_crdt_mutation_response
  """
  delete single row from the table: "crdt"
  """
  hasura_delete_crdt_by_pk(id: String!): hasura_crdt
  """
  insert data into the table: "authors"
  """
  hasura_insert_authors(
    """the rows to be inserted"""
    objects: [hasura_authors_insert_input!]!
    """upsert condition"""
    on_conflict: hasura_authors_on_conflict
  ): hasura_authors_mutation_response
  """
  insert a single row into the table: "authors"
  """
  hasura_insert_authors_one(
    """the row to be inserted"""
    object: hasura_authors_insert_input!
    """upsert condition"""
    on_conflict: hasura_authors_on_conflict
  ): hasura_authors
  """
  insert data into the table: "books"
  """
  hasura_insert_books(
    """the rows to be inserted"""
    objects: [hasura_books_insert_input!]!
    """upsert condition"""
    on_conflict: hasura_books_on_conflict
  ): hasura_books_mutation_response
  """
  insert a single row into the table: "books"
  """
  hasura_insert_books_one(
    """the row to be inserted"""
    object: hasura_books_insert_input!
    """upsert condition"""
    on_conflict: hasura_books_on_conflict
  ): hasura_books
  """
  insert data into the table: "crdt"
  """
  hasura_insert_crdt(
    """the rows to be inserted"""
    objects: [hasura_crdt_insert_input!]!
    """upsert condition"""
    on_conflict: hasura_crdt_on_conflict
  ): hasura_crdt_mutation_response
  """
  insert a single row into the table: "crdt"
  """
  hasura_insert_crdt_one(
    """the row to be inserted"""
    object: hasura_crdt_insert_input!
    """upsert condition"""
    on_conflict: hasura_crdt_on_conflict
  ): hasura_crdt
  """
  update data of the table: "authors"
  """
  hasura_update_authors(
    """sets the columns of the filtered rows to the given values"""
    _set: hasura_authors_set_input
    """filter the rows which have to be updated"""
    where: hasura_authors_bool_exp!
  ): hasura_authors_mutation_response
  """
  update single row of the table: "authors"
  """
  hasura_update_authors_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: hasura_authors_set_input
    pk_columns: hasura_authors_pk_columns_input!
  ): hasura_authors
  """
  update multiples rows of table: "authors"
  """
  hasura_update_authors_many(
    """updates to execute, in order"""
    updates: [hasura_authors_updates!]!
  ): [hasura_authors_mutation_response]
  """
  update data of the table: "books"
  """
  hasura_update_books(
    """sets the columns of the filtered rows to the given values"""
    _set: hasura_books_set_input
    """filter the rows which have to be updated"""
    where: hasura_books_bool_exp!
  ): hasura_books_mutation_response
  """
  update single row of the table: "books"
  """
  hasura_update_books_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: hasura_books_set_input
    pk_columns: hasura_books_pk_columns_input!
  ): hasura_books
  """
  update multiples rows of table: "books"
  """
  hasura_update_books_many(
    """updates to execute, in order"""
    updates: [hasura_books_updates!]!
  ): [hasura_books_mutation_response]
  """
  update data of the table: "crdt"
  """
  hasura_update_crdt(
    """sets the columns of the filtered rows to the given values"""
    _set: hasura_crdt_set_input
    """filter the rows which have to be updated"""
    where: hasura_crdt_bool_exp!
  ): hasura_crdt_mutation_response
  """
  update single row of the table: "crdt"
  """
  hasura_update_crdt_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: hasura_crdt_set_input
    pk_columns: hasura_crdt_pk_columns_input!
  ): hasura_crdt
  """
  update multiples rows of table: "crdt"
  """
  hasura_update_crdt_many(
    """updates to execute, in order"""
    updates: [hasura_crdt_updates!]!
  ): [hasura_crdt_mutation_response]
}

"""column ordering options"""
enum hasura_order_by {
  """in ascending order, nulls last"""
  asc
  """in ascending order, nulls first"""
  asc_nulls_first
  """in ascending order, nulls last"""
  asc_nulls_last
  """in descending order, nulls first"""
  desc
  """in descending order, nulls first"""
  desc_nulls_first
  """in descending order, nulls last"""
  desc_nulls_last
}

type Query {
  """
  fetch data from the table: "authors"
  """
  hasura_authors(
    """distinct select on columns"""
    distinct_on: [hasura_authors_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_authors_order_by!]
    """filter the rows returned"""
    where: hasura_authors_bool_exp
  ): [hasura_authors!]!
  """
  fetch aggregated fields from the table: "authors"
  """
  hasura_authors_aggregate(
    """distinct select on columns"""
    distinct_on: [hasura_authors_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_authors_order_by!]
    """filter the rows returned"""
    where: hasura_authors_bool_exp
  ): hasura_authors_aggregate!
  """fetch data from the table: "authors" using primary key columns"""
  hasura_authors_by_pk(id: String!): hasura_authors
  """An array relationship"""
  hasura_books(
    """distinct select on columns"""
    distinct_on: [hasura_books_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_books_order_by!]
    """filter the rows returned"""
    where: hasura_books_bool_exp
  ): [hasura_books!]!
  """An aggregate relationship"""
  hasura_books_aggregate(
    """distinct select on columns"""
    distinct_on: [hasura_books_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_books_order_by!]
    """filter the rows returned"""
    where: hasura_books_bool_exp
  ): hasura_books_aggregate!
  """fetch data from the table: "books" using primary key columns"""
  hasura_books_by_pk(id: String!): hasura_books
  """
  fetch data from the table: "crdt"
  """
  hasura_crdt(
    """distinct select on columns"""
    distinct_on: [hasura_crdt_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_crdt_order_by!]
    """filter the rows returned"""
    where: hasura_crdt_bool_exp
  ): [hasura_crdt!]!
  """
  fetch aggregated fields from the table: "crdt"
  """
  hasura_crdt_aggregate(
    """distinct select on columns"""
    distinct_on: [hasura_crdt_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_crdt_order_by!]
    """filter the rows returned"""
    where: hasura_crdt_bool_exp
  ): hasura_crdt_aggregate!
  """fetch data from the table: "crdt" using primary key columns"""
  hasura_crdt_by_pk(id: String!): hasura_crdt
  public_hello: String
}

type Subscription {
  """
  fetch data from the table: "authors"
  """
  hasura_authors(
    """distinct select on columns"""
    distinct_on: [hasura_authors_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_authors_order_by!]
    """filter the rows returned"""
    where: hasura_authors_bool_exp
  ): [hasura_authors!]!
  """
  fetch aggregated fields from the table: "authors"
  """
  hasura_authors_aggregate(
    """distinct select on columns"""
    distinct_on: [hasura_authors_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_authors_order_by!]
    """filter the rows returned"""
    where: hasura_authors_bool_exp
  ): hasura_authors_aggregate!
  """fetch data from the table: "authors" using primary key columns"""
  hasura_authors_by_pk(id: String!): hasura_authors
  """
  fetch data from the table in a streaming manner: "authors"
  """
  hasura_authors_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!
    """cursor to stream the results returned by the query"""
    cursor: [hasura_authors_stream_cursor_input]!
    """filter the rows returned"""
    where: hasura_authors_bool_exp
  ): [hasura_authors!]!
  """An array relationship"""
  hasura_books(
    """distinct select on columns"""
    distinct_on: [hasura_books_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_books_order_by!]
    """filter the rows returned"""
    where: hasura_books_bool_exp
  ): [hasura_books!]!
  """An aggregate relationship"""
  hasura_books_aggregate(
    """distinct select on columns"""
    distinct_on: [hasura_books_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_books_order_by!]
    """filter the rows returned"""
    where: hasura_books_bool_exp
  ): hasura_books_aggregate!
  """fetch data from the table: "books" using primary key columns"""
  hasura_books_by_pk(id: String!): hasura_books
  """
  fetch data from the table in a streaming manner: "books"
  """
  hasura_books_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!
    """cursor to stream the results returned by the query"""
    cursor: [hasura_books_stream_cursor_input]!
    """filter the rows returned"""
    where: hasura_books_bool_exp
  ): [hasura_books!]!
  """
  fetch data from the table: "crdt"
  """
  hasura_crdt(
    """distinct select on columns"""
    distinct_on: [hasura_crdt_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_crdt_order_by!]
    """filter the rows returned"""
    where: hasura_crdt_bool_exp
  ): [hasura_crdt!]!
  """
  fetch aggregated fields from the table: "crdt"
  """
  hasura_crdt_aggregate(
    """distinct select on columns"""
    distinct_on: [hasura_crdt_select_column!]
    """limit the number of rows returned"""
    limit: Int
    """skip the first n rows. Use only with order_by"""
    offset: Int
    """sort the rows by one or more columns"""
    order_by: [hasura_crdt_order_by!]
    """filter the rows returned"""
    where: hasura_crdt_bool_exp
  ): hasura_crdt_aggregate!
  """fetch data from the table: "crdt" using primary key columns"""
  hasura_crdt_by_pk(id: String!): hasura_crdt
  """
  fetch data from the table in a streaming manner: "crdt"
  """
  hasura_crdt_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!
    """cursor to stream the results returned by the query"""
    cursor: [hasura_crdt_stream_cursor_input]!
    """filter the rows returned"""
    where: hasura_crdt_bool_exp
  ): [hasura_crdt!]!
  public_hello: String
}

scalar hasura_timestamptz

"""
Boolean expression to compare columns of type "timestamptz". All fields are combined with logical 'AND'.
"""
input hasura_timestamptz_comparison_exp {
  _eq: hasura_timestamptz
  _gt: hasura_timestamptz
  _gte: hasura_timestamptz
  _in: [hasura_timestamptz!]
  _is_null: Boolean
  _lt: hasura_timestamptz
  _lte: hasura_timestamptz
  _neq: hasura_timestamptz
  _nin: [hasura_timestamptz!]
}

enum Claim {
  USERID
  EMAIL
  EMAIL_VERIFIED
  NAME
  NICKNAME
  LOCATION
  PROVIDER
}

enum COMMON_REGEX_PATTERN {
  EMAIL
  DOMAIN
}

enum WG_ROLE {
  admin
  user
}

enum WunderGraphDateTimeFormat {
  """2006-01-02T15:04:05-0700"""
  ISO8601
  """Mon Jan _2 15:04:05 2006"""
  ANSIC
  """Mon Jan _2 15:04:05 MST 2006"""
  UnixDate
  """Mon Jan 02 15:04:05 -0700 2006"""
  RubyDate
  """02 Jan 06 15:04 MST"""
  RFC822
  """02 Jan 06 15:04 -0700"""
  RFC822Z
  """Monday, 02-Jan-06 15:04:05 MST"""
  RFC850
  """Mon, 02 Jan 2006 15:04:05 MST"""
  RFC1123
  """Mon, 02 Jan 2006 15:04:05 -0700"""
  RFC1123Z
  """2006-01-02T15:04:05Z07:00"""
  RFC3339
  """2006-01-02T15:04:05.999999999Z07:00"""
  RFC3339Nano
  """3:04PM"""
  Kitchen
  """Jan _2 15:04:05"""
  Stamp
  """Jan _2 15:04:05.000"""
  StampMilli
  """Jan _2 15:04:05.000000"""
  StampMicro
  """Jan _2 15:04:05.000000000"""
  StampNano
}